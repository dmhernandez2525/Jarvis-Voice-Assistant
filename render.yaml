# Jarvis Voice Assistant - Render Infrastructure Configuration
# ============================================================
# Static landing page + Flask API server for remote voice assistant access.
#
# IMPORTANT NOTES:
# - Website: Static landing page explaining the project
# - API Server: Deploys voice_assistant_server.py (requires external Ollama)
# - Whisper runs on the server (CPU-intensive, may be slow on free tier)
#
# LIMITATIONS (as of January 2026):
# - Free web services spin down after 15 minutes of inactivity
# - No GPU support on Render (Whisper will use CPU)
# - 512MB RAM limit on free tier (may not fit large Whisper models)
#
# RECOMMENDATION: Deploy API on a machine with GPU for production use.
# This config is for demo/testing purposes.

services:
  # =============================================
  # Static Site - Landing Page
  # =============================================
  - type: web
    name: jarvis-site
    env: static
    buildCommand: cd website && npm ci && npm run build
    staticPublishPath: ./website/dist
    envVars:
      - key: NODE_VERSION
        value: "20"
    headers:
      - path: /*
        name: X-Frame-Options
        value: DENY
      - path: /*
        name: X-Content-Type-Options
        value: nosniff
      - path: /*
        name: Referrer-Policy
        value: strict-origin-when-cross-origin
    routes:
      - type: rewrite
        source: /*
        destination: /index.html

  # =============================================
  # API Server - Flask Voice Assistant (Stub)
  # NOTE: Full voice processing requires Docker deployment with ffmpeg/portaudio
  # =============================================
  - type: web
    name: jarvis-voice-api
    runtime: python
    plan: free  # Change to 'starter' for production
    buildCommand: |
      pip install --upgrade pip
      pip install -r requirements-render.txt
    startCommand: python voice_assistant_server_render.py
    healthCheckPath: /health
    envVars:
      - key: PYTHON_VERSION
        value: "3.11"
      - key: PORT
        value: "5000"
      # Ollama configuration (external instance required)
      - key: OLLAMA_HOST
        value: UPDATE_ME  # e.g., http://your-ollama-server:11434
      - key: OLLAMA_MODEL
        value: "qwen2.5:7b"  # Use smaller model for limited resources
      # Whisper configuration
      - key: WHISPER_MODEL
        value: "base"  # Use 'base' or 'small' for free tier RAM limits
      # CORS configuration
      - key: CORS_ORIGINS
        value: "*"
      # Optional: API authentication
      - key: API_KEY
        generateValue: true

# NOTE: The API service requires an external Ollama instance.
# Ollama cannot run on Render due to model size and GPU requirements.
# Options:
# 1. Run Ollama on your own server/VM with GPU
# 2. Use a cloud GPU provider (RunPod, Lambda Labs, etc.)
# 3. Use OpenAI API instead (modify code to use openai package)
